---
title: 'faster : appendix'
author: "Hyeongchan Bae"
date: "April 2022"
output:
  pdf_document:
    toc: true
    latex_engine: xelatex
  html_document:
    toc: true
    toc_float: true
    theme: yeti
mainfont: NanumGothic
monofont: NanumGothic
fontsize: 12
linestretch: 1.5  
---

## 1. bsi

제조업경기조사 원자료 $\to$ 보도자료용 부표, 주관식 응답 만들기 

```{r, eval = FALSE}
# bsi

# 22.04.15 updated

# 0. what do you need

FOLDER = paste0('C:/Users/', Sys.info()['user'], '/Documents/GitHub/KIET_831/bsi')

YEAR = 2022
QUARTER = 1
FILE = '케이스탯(2022.03) - 제조업 경기조사 및 패널기업 관리 결과표(2022년 1분기)_220404.xlsx'

# 1. setting

setwd(FOLDER)

library(tidyverse)
library(readxl)
library(openxlsx)

ROUND <- function(x, digits = 0) {
  
  posneg = sign(x)
  
  z = abs(x)*10^digits
  z = z + 0.5 + sqrt(.Machine$double.eps)
  z = trunc(z)
  z = z/10^digits
  
  z*posneg
  
} # 기본 function은 round(0.5) = 0 만들어서, ROUND(0.5) = 1 되는 사용자 함수 생성

# 2. data load

BSI_SECTOR <- read_excel(FILE, sheet = '업종별 BSI')

temp1 <- read_excel(FILE, sheet = '유형별 BSI')

colnames(temp1) <- temp1[1, ]

BSI_TYPE <- temp1 %>% 
  slice(-1) %>%
  mutate_at(vars(ICT:수출기업), as.double)

temp2 <- read_excel(FILE, sheet = '유형별 개별지수')

colnames(temp2) <- temp2[1, ]

BSI_DETAIL <- temp2 %>% 
  slice(-1) %>% 
  mutate_at(vars(반도체:이차전지), as.double)

BSI_REGION <- read_excel(FILE, sheet = '지역별 BSI')

# 3. 보도자료용 부표

# 3-1. 응답 업체 구성(p.2)

temp3 <- BSI_SECTOR %>% filter(산업 == '응답기업수')

temp4 <- BSI_TYPE %>% filter(산업 == '응답기업수')

ORDER <- c('산업', '반도체', '디스플레이', '무선통신기기', '가전',
           '자동차', '조선', '일반기계',
           '정유', '화학', '철강', '섬유',
           '바이오/헬스', '이차전지',
           '제조업 전체',
           'ICT', '기계', '소재', '신산업',
           '대기업', '중소기업', '내수기업', '수출기업') # 부표 내 산업 순서

APPENDIX.1 <- right_join(temp3, temp4) %>% 
  relocate(all_of(ORDER)) %>% 
  pivot_longer(cols = 2:23) %>% 
  select(-1) %>% 
  mutate(비중 = ROUND(value / 1000 * 100, digits = 1)) %>% 
  select(name, 비중)

# 3-2. 기상도(p.3-4)

temp5 <- BSI_SECTOR %>% slice(3:14)

temp6 <- BSI_TYPE %>% slice(2:13)

temp7 <- right_join(temp5, temp6) %>% 
  relocate(all_of(ORDER)) %>% 
  relocate('제조업 전체', .after = '산업') %>% 
  t() %>% 
  as.data.frame() %>%
  rownames_to_column() %>% 
  tibble()

colnames(temp7) <- temp7[1, ]

BSI_CUT.1 <- temp7 %>% 
  slice(-1) %>% 
  mutate_at(vars(시황:`자금사정@1`), as.double) %>% 
  mutate_at(vars(시황:`자금사정@1`), ROUND) # longer 현황

APPENDIX.2 <- BSI_CUT.1 %>% 
  select('산업', '매출액@1', '국내시장출하@1', '수출@1', 
         '경상이익@1', '설비투자@1', '고용@1')

temp8 <- BSI_SECTOR %>% slice(15:26)

temp9 <- BSI_TYPE %>% slice(14:25)

temp10 <- right_join(temp8, temp9) %>% 
  relocate(all_of(ORDER)) %>% 
  relocate('제조업 전체', .after = '산업') %>% 
  t() %>% 
  as.data.frame() %>%
  rownames_to_column() %>% 
  tibble()

colnames(temp10) <- temp10[1, ]

BSI_CUT.2 <- temp10 %>% 
  slice(-1) %>% 
  mutate_at(vars(`시황전망@2`:`자금사정전망@2`), as.double) %>% 
  mutate_at(vars(`시황전망@2`:`자금사정전망@2`), ROUND) # longer 전망

APPENDIX.3 <- BSI_CUT.2 %>% 
  select('산업', '매출액전망@2', '국내시장출하전망@2', '수출전망@2', 
         '경상이익전망@2', '설비투자전망@2', '고용전망@2')

# 3-3. 제조업 전체 및 분류별 통계(p.5-16)

temp11 <- BSI_CUT.1 %>% 
  t() %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  tibble

colnames(temp11) <- temp11[1, ]

BSI_CUT.3 <- temp11 %>% 
  slice(-1) %>% 
  mutate_at(vars(`제조업 전체`:수출기업), as.double) %>% 
  rename(구분 = 산업) # wider 현황

temp12 <- BSI_CUT.3 %>% 
  select('구분', '제조업 전체', 
         'ICT', '기계', '소재', '신산업', 
         '대기업', '중소기업')

temp13 <- BSI_CUT.2 %>% 
  t() %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  tibble

colnames(temp13) <- temp13[1, ]

BSI_CUT.4 <- temp13 %>% 
  slice(-1) %>% 
  mutate_at(vars(`제조업 전체`:수출기업), as.double) %>% 
  rename(구분 = 산업) # wider 전망

temp14 <- BSI_CUT.4 %>% 
  select('구분', '제조업 전체', 
         'ICT', '기계', '소재', '신산업', 
         '대기업', '중소기업')

APPENDIX.4 <- rbind(temp12, temp14)

# 3-4. 세부 업종별 조사 통계(p.17-28)

temp15 <- BSI_CUT.3 %>% select('구분', ORDER[2:14])

temp16 <- BSI_CUT.4 %>% select('구분', ORDER[2:14])

APPENDIX.5 <- rbind(temp15, temp16)

# 4. 주관식 답변(산업부 제공)

BSI_RAWDATA <- read_xlsx(FILE, sheet = 7)

temp17 <- BSI_RAWDATA %>% 
  select(COM, JCODE, JCODE1, Q5_M1, Q5_M1_ET, Q8_M1, Q8_M1_ET) %>% 
  mutate(업종 = case_when(JCODE == 1 ~ '반도체', 
                          JCODE == 2 ~ '디스플레이',
                          JCODE == 3 ~ '가전',
                          JCODE == 4 ~ '무선통신기기',
                          JCODE == 5 ~ '정유',
                          JCODE == 6 ~ '화학',
                          JCODE == 7 ~ '철강',
                          JCODE == 8 ~ '섬유',
                          JCODE == 9 ~ '일반기계',
                          JCODE == 10 ~ '자동차',
                          JCODE == 11 ~ '조선', 
                          JCODE == 12 ~ '바이오/헬스',
                          JCODE == 13 ~ '이차전지')) %>% 
  mutate(유형 = case_when(JCODE1 == 1 ~ 'ICT',
                          JCODE1 == 2 ~ '소재',
                          JCODE1 == 3 ~ '기계',
                          JCODE1 == 4 ~ '신산업')) %>%
  arrange(JCODE)

# 4-1. 매출액(전망) 사유

COMMENT_BAD <- tibble()

for (i in 1:13) {
  
  temp18 <- temp17 %>% 
    filter(JCODE == i & Q5_M1 %in% 1:3) %>% 
    select(JCODE, 업종, JCODE1, 유형, Q5_M1_ET)
  
  COMMENT_BAD <- rbind(COMMENT_BAD, temp18)
  
}

COMMENT_SOSO <- tibble()

for (i in 1:13) {
  
  temp19 <- temp17 %>% 
    filter(JCODE == i & Q5_M1 %in% 4) %>% 
    select(JCODE, 업종, JCODE1, 유형, Q5_M1_ET)
  
  COMMENT_SOSO <- rbind(COMMENT_SOSO, temp19)
  
}

COMMENT_GOOD <- tibble()

for (i in 1:13) {
  
  temp20 <- temp17 %>% 
    filter(JCODE == i & Q5_M1 %in% 5:7) %>% 
    select(JCODE, 업종, JCODE1, 유형, Q5_M1_ET)
  
  COMMENT_GOOD <- rbind(COMMENT_GOOD, temp20)
  
}

colnames(COMMENT_BAD)[5] <- '부정'
colnames(COMMENT_SOSO)[5] <- '불변'
colnames(COMMENT_GOOD)[5] <- '긍정'

COMMENT_BAD.1 <- COMMENT_BAD %>% 
  select(유형, 업종, 부정) %>%
  arrange(유형, 업종, 부정) # 매출액 부정 사유

COMMENT_SOSO.1 <- COMMENT_SOSO %>% 
  select(유형, 업종, 불변) %>% 
  arrange(유형, 업종, 불변)

COMMENT_GOOD.1 <- COMMENT_GOOD %>%
  select(유형, 업종, 긍정) %>%
  arrange(유형, 업종, 긍정) # 매출액 긍정 사유

# 4-2. 매출전망 사유

COMMENT_BAD <- tibble()

for (i in 1:13) {
  
  temp18 <- temp17 %>% 
    filter(JCODE == i & Q8_M1 %in% 1:3) %>% 
    select(JCODE, 업종, JCODE1, 유형, Q8_M1_ET)
  
  COMMENT_BAD <- rbind(COMMENT_BAD, temp18)
  
}

COMMENT_SOSO <- tibble()

for (i in 1:13) {
  
  temp19 <- temp17 %>% 
    filter(JCODE == i & Q8_M1 %in% 4) %>% 
    select(JCODE, 업종, JCODE1, 유형, Q8_M1_ET)
  
  COMMENT_SOSO <- rbind(COMMENT_SOSO, temp19)
  
}

COMMENT_GOOD <- tibble()

for (i in 1:13) {
  
  temp20 <- temp17 %>% 
    filter(JCODE == i & Q8_M1 %in% 5:7) %>% 
    select(JCODE, 업종, JCODE1, 유형, Q8_M1_ET)
  
  COMMENT_GOOD <- rbind(COMMENT_GOOD, temp20)
  
}

colnames(COMMENT_BAD)[5] <- '부정'
colnames(COMMENT_SOSO)[5] <- '불변'
colnames(COMMENT_GOOD)[5] <- '긍정'

COMMENT_BAD.2 <- COMMENT_BAD %>% 
  select(유형, 업종, 부정) %>%
  arrange(유형, 업종, 부정) # 매출전망 부정 사유

COMMENT_SOSO.2 <- COMMENT_SOSO %>% 
  select(유형, 업종, 불변) %>% 
  arrange(유형, 업종, 불변)

COMMENT_GOOD.2 <- COMMENT_GOOD %>%
  select(유형, 업종, 긍정) %>%
  arrange(유형, 업종, 긍정) # 매출전망 긍정 사유

rm(list = c('COMMENT_BAD', 'COMMENT_SOSO', 'COMMENT_GOOD'))

COMMENT_BAD.1$부정 <- COMMENT_BAD.1$부정 %>% str_replace_all('\n', '')
COMMENT_SOSO.1$불변 <- COMMENT_SOSO.1$불변 %>% str_replace_all('\n', '')
COMMENT_GOOD.1$긍정 <- COMMENT_GOOD.1$긍정 %>% str_replace_all('\n', '')
COMMENT_BAD.2$부정 <- COMMENT_BAD.2$부정 %>% str_replace_all('\n', '')
COMMENT_SOSO.2$불변 <- COMMENT_SOSO.2$불변 %>% str_replace_all('\n', '')
COMMENT_GOOD.2$긍정 <- COMMENT_GOOD.2$긍정 %>% str_replace_all('\n', '')

# 5. 보고서용 부표

# 5-1. 업종 BSI (내수, 수출기업만 추가)

APPENDIX.TYPE <- rbind(
  
  BSI_CUT.3 %>% 
  select('구분', '제조업 전체', 
         'ICT', '기계', '소재', '신산업', 
         '대기업', '중소기업',
         '내수기업', '수출기업'),
  
  BSI_CUT.4 %>% 
    select('구분', '제조업 전체', 
           'ICT', '기계', '소재', '신산업', 
           '대기업', '중소기업',
           '내수기업', '수출기업')
  
  ) # 유형별 시계열

APPENDIX.SECTOR <- APPENDIX.5 # 업종별 시계열

# 5-2. 지역 BSI

APPENDIX.REGION <- BSI_REGION %>% 
  slice(3:26) %>% 
  relocate(지역, 전국,
           서울, 인천, 경기, 강원,
           부산, 울산, 경남,
           대구, 경북,
           광주, 전북, 전남,
           대전, 충북, 충남) %>% 
  mutate_at(vars(전국:충남), ROUND) # 지역별 시계열

# 5-3. 중국 BSI

# 열심히 개발 예정

# 6. export

FOLDER.2 <- paste0(FOLDER, '/', YEAR, '-', QUARTER, 'Q')

dir.create(FOLDER.2)

setwd(FOLDER.2)

write.xlsx(x = list(APPENDIX.1, APPENDIX.2, APPENDIX.3, APPENDIX.4, APPENDIX.5),
           sheetName = c('응답 업체 구성(p.2)', '현황 BSI(p.3)', '전망 BSI(p.4)',
                         '제조업 전체 및 분류별 통계(p.5-16)',
                         '세부 업종별 조사 통계(p.17-28)'),
           file = paste0(YEAR, '-', QUARTER, 'Q BSI 보도자료용 부표.xlsx')) # 3. 보도자료용 부표

write.xlsx(x = list(COMMENT_BAD.1, COMMENT_GOOD.1, COMMENT_BAD.2, COMMENT_GOOD.2),
           sheetName = c('매출액 부정', '매출액 긍정',
                         '매출전망 부정', '매출전망 긍정'),
           file = '주관식.xlsx') # 4. 주관식 답변(산업부 제공)

write.xlsx(x = list(APPENDIX.TYPE, APPENDIX.SECTOR),
           sheetName = c('유형별 시계열', '업종별 시계열'),
           file = paste0(YEAR, '-', QUARTER, 'Q 업종 BSI 보고서용 부표.xlsx')) # 5-1. 업종 BSI

write.xlsx(x = list(APPENDIX.REGION),
           sheetName = c('지역별 시계열'),
           file = paste0(YEAR, '-', QUARTER, 'Q 지역 BSI 보고서용 부표.xlsx')) # 5-2. 지역 BSI
```

## 2. psi

전문가경기서베이조사 원자료 $\to$ 보도자료용 부표 만들기

```{r, eval = FALSE}
# psi

# 22.04.15 updated

# 0. what do you need

FOLDER = paste0('C:/Users/', Sys.info()['user'], '/Documents/GitHub/KIET_831/psi')

YEAR = 2022
MONTH = 3
FILE = '2022년3월_누적시계열_PSI_작성용(잠정).xlsx' # 최신 버전 누적시계열 파일

# 1. setting

setwd(FOLDER)

library(tidyverse) # 데이터 핸들링 패키지
library(readxl) # 엑셀 로드 패키지
library(openxlsx) # 엑셀 출력 패키지

ROUND <- function(x, digits = 0) {
  
  posneg = sign(x)
  
  z = abs(x)*10^digits
  z = z + 0.5 + sqrt(.Machine$double.eps)
  z = trunc(z)
  z = z/10^digits
  
  z*posneg
  
} # 기본 function은 round(0.5) = 0 만들어서, ROUND(0.5) = 1 되는 사용자 함수 생성

# 2. data load

PSI_SIMPLE <- read_xlsx(FILE, '종합(단순평균)', na = c('', NA)) # 누적시계열의 1번 시트
PSI_WEIGHTED <- read_xlsx(FILE, '종합(가중평균)', na = c('', NA)) # 누적시계열의 2번 시트

PSI_SIMPLE_CUT <- PSI_SIMPLE %>% filter(연도 == YEAR, 월 == MONTH) # 필요 시점 데이터만 선택
PSI_WEIGHTED_CUT <- PSI_WEIGHTED %>% filter(연도 == YEAR, 월 == MONTH) # 필요 시점 데이터만 선택

# 3. make table

# 3-1. 업종별 패널 구성(p.2)

temp1 <- PSI_SIMPLE_CUT %>% # 2022년 2월 단순평균 데이터에서 다음 작업을 한 뒤 temp1 임시 객체로 저장
  select(구분, 응답수) %>% # 구분, 응답수 column만 골라서
  mutate(`응답자 구성비(%)` = ROUND(응답수 / PSI_SIMPLE_CUT$응답수[1] * 100, digits = 1)) # 소수점 한 자리로 구성비 계산하고 변수 생성

APPENDIX.1 <- temp1[c(9, 8, 5, 7, 6, 11, 12, 10, 15, 14, 13, 16, 17, 18), ] %>% # 부표 양식대로 산업 순서 바꾸고
  select(1, 3) # 구분, 응답자 구성비만 저장

# 3-2. 기상도(p.3-4)

temp2 <- PSI_SIMPLE_CUT %>% # 단순평균 데이터에서
  select(경기현황, 시장판매현황, 수출현황, 생산수준현황, 투자액현황, 채산성현황) %>% # 항목(변수)을 고른 뒤
  ROUND() %>% # 반올림 해주고
  mutate(구분 = PSI_SIMPLE_CUT$구분) %>% # 반올림 하느라 빼두었던 character variable 다시 넣고
  relocate(구분) # 순서도 맨 앞으로 조정

APPENDIX.2 <- temp2[c(1, 9, 8, 5, 7, 6, 11, 12, 10, 15, 14, 13, 16, 2, 3, 4), ] # 부표 양식 맞춰서 저장

temp3 <- PSI_SIMPLE_CUT %>% 
  select(경기전망, 시장판매전망, 수출전망, 생산수준전망, 투자액전망, 채산성전망) %>% 
  ROUND() %>% 
  mutate(구분 = PSI_SIMPLE_CUT$구분) %>% 
  relocate(구분)

APPENDIX.3 <- temp3[c(1, 9, 8, 5, 7, 6, 11, 12, 10, 15, 14, 13, 16, 2, 3, 4), ] # APPENDIX.2와 같은 방식으로 전망 파트 작업

# 3-3. 제조업 및 부문별 통계(p.5-12)

temp4 <- PSI_SIMPLE_CUT %>% # 단순평균 데이터에서
  select(-c(1:4)) %>% # 1~4번째 column(연도, 월, 구분, 응답수) 제외하고, 즉 psi 값만 대상으로
  ROUND() %>% # 반올림 해주고
  mutate(구분 = PSI_SIMPLE_CUT$구분) %>% # 빼두었던 산업 구분 넣고
  relocate(구분) %>% # 맨 앞으로 옮겨준 다음
  filter(구분 %in% c('00_전체', '01_ICT', '02_장비', '03_소재')) %>% # 전체, ICT, 장비, 소재 행만 선택
  select(구분, starts_with(c('경기', '시장', '수출', '생산', '재고', '투자', '채산성', '제품단가'))) # 산업 구분과 부표 파트인 열만 선택 

temp4$재고수준현황 <- abs(temp4$재고수준현황 - 200) # 재고수준은 200 빼고 절대값 취해서 계산
temp4$재고수준전망 <- abs(temp4$재고수준전망 - 200)

temp5 <- PSI_WEIGHTED_CUT %>% # 가중평균 데이터에서도 비슷한 작업을 하는데
  select(-c(1:4)) %>% 
  ROUND() %>% 
  mutate(구분 = PSI_SIMPLE_CUT$구분) %>% 
  relocate(구분) %>% 
  filter(구분 %in% c('00_전체')) %>% # 제조업(전체)만 필요하니까 하나 선택
  select(구분, starts_with(c('경기', '시장', '수출', '생산', '재고', '투자', '채산성', '제품단가')))

temp5$재고수준현황 <- abs(temp5$재고수준현황 - 200)
temp5$재고수준전망 <- abs(temp5$재고수준전망 - 200)

temp5[1, 1] <- '00_전체_가중' # 이름 같으면 헷갈리니까 가중지수는 따로 네이밍

temp6 <- rbind(temp4, temp5) # 단순평균, 가중평균 데이터에서 뽑아낸 4개, 1개 열을 묶고

temp7 <- temp6[c(1, 5, 2, 3, 4), ] %>% # 열 순서 조정한 다음
  t() %>% # 행열 뒤집고 (부표 스타일 보니, 가져다 붙이려면 뒤집는 게 좋아보임)
  as.data.frame() %>% # 데이터 프레임 형식으로 바꾸고 (matrix와 비슷)
  rownames_to_column() %>% # rowname으로 내려온 파트 이름을 아예 column 으로 만들고
  tibble() %>% # 조금 더 정제된 데이터 프레임 형식 전환
  filter(rowname != '구분') # 첫 줄은 거추장스러워서 제거

names(temp7) <- c('파트', '단순지수', '가중지수', 'ICT부문', '기계부문', '소재부문') # 네이밍 수정

APPENDIX.4 <- temp7 %>% 
  mutate_at(vars(단순지수:소재부문), as.double) # 변환하면서 character로 입력된 숫자값을 number class로 바꾸고 저장

# 3-4. 세부 업종별 조사 통계(p.13-20)

temp8 <- PSI_SIMPLE_CUT %>% # 대충 비슷하니까 이하는 안써도 되겠지?
  select(-c(1:4)) %>% 
  ROUND() %>% 
  mutate(구분 = PSI_SIMPLE_CUT$구분) %>% 
  relocate(구분) %>%
  filter(구분 %in% c('08_반도체', '07_디스플레이', '06_핸드폰', '05_가전', '10_자동차',
                   '11_조선', '09_기계', '14_화학', '13_철강', '12_섬유', '15_바이오헬스')) %>% 
  select(구분, starts_with(c('경기', '시장', '수출', '생산', '재고', '투자', '채산성', '제품단가')))

temp8$재고수준현황 <- abs(temp8$재고수준현황 - 200)
temp8$재고수준전망 <- abs(temp8$재고수준전망 - 200)

temp9 <- temp8 %>%
  t() %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  tibble() 

temp10 <- temp9[-1, c(1, 5, 4, 3, 2, 7, 8, 6, 11, 10, 9, 12)]

names(temp10) <- c('파트', '08_반도체', '07_디스플레이', '06_핸드폰', '05_가전', '10_자동차',
                   '11_조선', '09_기계', '14_화학', '13_철강', '12_섬유', '15_바이오헬스')

APPENDIX.5 <- temp10 %>% 
  mutate_at(vars(`08_반도체`:`15_바이오헬스`), as.double)# 세부 업종별 조사 통계

# 5. export

FOLDER.2 <- paste0(FOLDER, '/', YEAR, '-', MONTH, 'M')

dir.create(FOLDER.2)

setwd(FOLDER.2)

write.xlsx(x = list(APPENDIX.1, APPENDIX.2, APPENDIX.3, APPENDIX.4, APPENDIX.5),
           sheetName = c('업종별 패널 구성(p.2)', '현황 PSI(p.3)', '전망 PSI(p.4)',
                         '제조업 및 부문별 통계(p.5-12)',
                         '세부 업종별 조사 통계(p.13-20)'),
           file = paste0(YEAR, '-', MONTH, 'M PSI 보도자료용 부표.xlsx'))
```

## 3. kosis

통계청 Open API 활용 (미완성)

```{r, eval = FALSE}
# kosis

# 22.04.15 updated

# 0. what do you need

FOLDER = paste0('C:/Users/', Sys.info()['user'], '/Documents/GitHub/KIET_831/kosis')

# 1. setting

setwd(FOLDER)

library(tidyverse)
library(jsonlite)
library(openxlsx)
library(lubridate)

BASE <- paste0('https://kosis.kr/openapi/statisticsData.do?method=getList', # 요청
               '&apiKey=시크릿시크릿', # 인증키
               '&format=json&jsonVD=Y', # 포맷 : JSON
               '&userStatsId=bhc5754/') # 방식 : 사용자가 기등록한 자료 로드

# 2. 광업제조업동향조사

# 2-1. 생산출하재고

CORE_생산출하재고 <- c('20220413144648', '20220413170357', '20220414092428') # 항목별 코드 (사용자 URL)

NUMBER_생산출하재고 <- 1:84

DATA_생산출하재고 <- tibble()

for (k in seq_along(CORE_생산출하재고)) {
  
  temp1 <- tibble()
  
  for (i in seq_along(NUMBER_생산출하재고)) {
    
    URL_생산출하재고 <- paste0(BASE,
                               '101/', # 통계청
                               'DT_1F01501/', # 시도/산업별 광공업생산지수(2015=100)
                               '2/', # 시계열
                               '1/', # 간격 : 1
                               CORE_생산출하재고[k], '_', NUMBER_생산출하재고[i], # URL 나열
                               '&prdSe=', 'M', # 주기 : Month
                               '&newEstPrdCnt=', '1') # 최근 1개 자료
    
    temp2 <- tryCatch(fromJSON(URL_생산출하재고) %>% 
                      tibble() %>% 
                      mutate(번호 = i) %>% 
                      select(번호, TBL_NM, ITM_ID, ITM_NM, PRD_DE, C1, C1_NM, C2, C2_NM, DT), # 필요 column 선택
                      error = function(e) tibble(NULL)) # 오류(데이터 부재) 발생하면 스킵
    
    temp1 <- rbind(temp2, temp1) # stacking
    
  }
  
  temp3 <- temp1 %>% 
    arrange(nchar(C2), C2) %>% # 분류값 순으로 정렬  
    mutate_at(vars(DT), as.double) # 수치값 class 숫자로 변경
  
  DATA_생산출하재고 <- rbind(temp3, DATA_생산출하재고) # stacking
  
}

temp4 <- DATA_생산출하재고 %>% split(as.factor(.$ITM_ID)) # 항목별 data frame 분리

temp5 <- DATA_생산출하재고$ITM_NM %>% unique() # 항목 이름

write.xlsx(temp4, sheetName = temp5, paste0(today(tzone = 'Asia/Seoul'), ' 생산출하재고.xlsx'))
```

## 4. news

언론사 페이지 접속해 뉴스 링크, 메타데이터 수집하기

```{r, eval = FALSE}
# news

# 22.04.15 updated

# 0. what do you need

PERIOD = 12 # 12시간 전 기사까지 스크랩(실제로는 좀 더 여유있게 모음)

FOLDER = paste0('C:/Users/', Sys.info()['user'], '/Documents/GitHub/KIET_831/news')

# 1. setting

setwd(FOLDER)

library(tidyverse)
library(rstudioapi)
library(RSelenium)
library(rvest)
library(lubridate)
library(openxlsx)

# 2. selenium

TERM_COMMAND <- 'java -Dwebdriver.gecko.driver="geckodriver.exe" -jar selenium-server-standalone-4.0.0-alpha-1.jar -port 4445'
terminalExecute(command = TERM_COMMAND) # 개인 컴퓨터에서 잘 안 되는 경우 java 설치, 방화벽 개인/공용 설정 권장

REMDR = remoteDriver(port = 4445, browserName = 'chrome')
REMDR$open()

# 3-1. 연합인포맥스(전체)

INFOMAX <- tibble()

for (i in 1:100) {
  
  URL_INFOMAX <- paste0('https://news.einfomax.co.kr/news/articleList.html?page=',
                        i, '&view_type=sm')
  
  REMDR$navigate(URL_INFOMAX)
  
  temp1 <- REMDR$getPageSource()
  
  temp2 <- read_html(temp1[[1]]) %>% 
    html_elements('div.list-titles') %>% 
    html_text() %>% 
    str_remove_all('\"') %>% 
    tibble() %>% 
    filter(str_detect(., ''))
  
  temp3 <- read_html(temp1[[1]]) %>% 
    html_elements('div.list-dated') %>% 
    html_text() %>%
    str_split(' | ', simplify = TRUE) %>% 
    as.data.frame() %>% 
    tibble() %>% 
    rownames_to_column(var = 'V0') %>% 
    mutate(V0 = as.double(V0))
  
  temp3.1 <- temp3 %>% 
    filter(V4 == '|') %>% 
    mutate(V7 = V6) %>% 
    mutate(V6 = V5)
  
  temp3.2 <- temp3 %>% 
    filter(V4 != '|')
  
  temp3.3 <- rbind(temp3.1, temp3.2) %>% 
    select(V0, V1, V3, V6, V7) %>% 
    arrange(V0) %>% 
    filter(V0 != 31) %>% 
    mutate(V8 = ymd_hm(paste(V6, V7), tz = 'Asia/Seoul')) %>% 
    select(V0, V1, V3, V8)
  
  temp4 <- read_html(temp1[[1]]) %>% 
    html_elements('p.list-summary') %>% 
    html_elements('a') %>% 
    html_attr('href')
  
  temp4.1 <- paste0('https://news.einfomax.co.kr', temp4) %>% 
    tibble %>% 
    filter(str_detect(., 'idxno'))
  
  temp5 <- tibble(제목 = temp2$., 분야 = temp3.3$V1, 기자 = temp3.3$V3, 시각 = temp3.3$V8, 링크 = temp4.1$.) %>% 
    relocate(분야, 시각, 기자, 링크, 제목)
  
  temp5.1 <- interval(temp5$시각, now(tz = 'Asia/Seoul')) %>% 
    as.period() < hours(PERIOD)
  
  if(sum(temp5.1) == 0) break
  
  INFOMAX <- rbind(INFOMAX, temp5)
  
}

INFOMAX <- INFOMAX %>% 
  mutate(시각 = as.character(시각))

# 3-2. 뉴스핌(글로벌)

NEWSPIM <- tibble()

for (i in 0:100) {
  
  URL_NEWSPIM <- paste0('https://www.newspim.com/news/lists/?category_cd=107&page=', i*20)
  
  REMDR$navigate(URL_NEWSPIM)
  
  temp6 <- REMDR$getPageSource()
  
  temp7 <- read_html(temp6[[1]]) %>% 
    html_elements('strong.subject_h') %>% 
    html_text() %>% 
    str_remove_all('\"') %>% 
    tibble() 
  
  temp8 <- read_html(temp6[[1]]) %>% 
    html_elements('p.byline') %>% 
    html_text() %>% 
    ymd_hm(tz = 'Asia/Seoul')
  
  temp9 <- read_html(temp6[[1]]) %>% 
    html_elements('article.thumbgroup') %>% 
    html_elements('a') %>% 
    html_attr('href') %>%
    tibble() %>% 
    distinct() %>% 
    slice(1:20)
  
  temp9.1 <- paste0('https://newspim.com', temp9$.) %>% 
    tibble()
  
  temp10 <- tibble(제목 = temp7$., 시각 = temp8, 링크 = temp9.1$.) %>% 
    relocate(시각, 링크, 제목)
  
  temp10.1 <- interval(temp10$시각, now(tz = 'Asia/Seoul')) %>% 
    as.period() < hours(PERIOD)
  
  if(sum(temp10.1) == 0) break
  
  NEWSPIM <- rbind(NEWSPIM, temp10)
  
}

NEWSPIM <- NEWSPIM %>% 
  mutate(시각 = as.character(시각))

# 3-3. 산업경제신문(전체)

EBN <- tibble()

for (i in 1:100) {
  
  URL_EBN <- paste0('https://www.ebn.co.kr/newslist?category1=99&page=', i)
  
  REMDR$navigate(URL_EBN)
  
  temp11 <- REMDR$getPageSource()
  
  temp12 <- read_html(temp11[[1]]) %>% 
    html_elements('h2.articleTitle') %>% 
    html_text() %>% 
    str_remove_all('\"') %>% 
    tibble() 
  
  temp13 <- read_html(temp11[[1]]) %>% 
    html_elements('p.articleInfo') %>% 
    html_text() %>% 
    str_remove_all('\n') %>% 
    str_remove_all(' ') %>% 
    str_remove_all('기자') %>% 
    tibble()
  
  temp13.1 <- temp13$. %>% 
    str_split('·', simplify = TRUE) %>% 
    as.data.frame() %>% 
    tibble()
  
  temp14 <- read_html(temp11[[1]]) %>% 
    html_elements('div.articleBox') %>% 
    html_elements('a') %>% 
    html_attr('href') %>% 
    tibble() %>% 
    distinct()
  
  temp14.1 <- paste0('https://ebn.co.kr', temp14$.) %>% 
    tibble()
  
  temp15 <- tibble(제목 = temp12$.,분야 = temp13.1$V1, 기자 = temp13.1$V2, 시각 = temp13.1$V3, 링크 = temp14.1$.) %>% 
    relocate(분야, 시각, 기자, 링크, 제목)
  
  temp15.1 <- temp15$시각 %in% paste0(1:PERIOD, '시간전')
  
  temp15.2 <- temp15$시각 %in% paste0(1:60, '분전')
  
  if(sum(temp15.1, temp15.2) == 0) break
  
  EBN <- rbind(EBN, temp15)
  
}

# 3-4. 글로벌이코노믹(국제)

GLOBALECO <- tibble()

for (i in 1:100) {
  
  URL_GLOBALECO <- paste0('https://www.g-enews.com/list.php?ct=g081100&pg=', i)
  
  REMDR$navigate(URL_GLOBALECO)
  
  temp16 <- REMDR$getPageSource()
  
  temp17 <- read_html(temp16[[1]]) %>% 
    html_elements('span.elip2') %>% 
    html_text() %>% 
    str_remove_all('\"') %>% 
    tibble() %>% 
    slice(3:15)
  
  temp18 <- read_html(temp16[[1]]) %>% 
    html_elements('p.e2') %>% 
    html_text() %>% 
    str_replace_all('\\.', '-') %>% 
    ymd_hm(tz = 'Asia/Seoul') %>% 
    tibble()
  
  temp19 <- read_html(temp16[[1]]) %>% 
    html_elements('a.e1') %>% 
    html_attr('href') %>% 
    tibble() %>% 
    filter(str_detect(., 'Global-Biz')) %>% 
    slice(1:13)
  
  temp20 <- tibble(제목 = temp17$., 시각 = temp18$., 링크 = temp19$.) %>% 
    relocate(시각, 링크, 제목)
  
  temp20.1 <- interval(temp20$시각, now(tz = 'Asia/Seoul')) %>% 
    as.period() < hours(PERIOD)
  
  if(sum(temp20.1) == 0) break
  
  GLOBALECO <- rbind(GLOBALECO, temp20)
  
}

GLOBALECO <- GLOBALECO %>% 
  mutate(시각 = as.character(시각))

# 3-5. 연합뉴스(세계)

YHNEWS <- tibble()

for (i in 1:100) {
  
  URL_YHNEWS <- paste0('https://www.yna.co.kr/international/all/', i)
  
  REMDR$navigate(URL_YHNEWS)
  
  temp21 <- REMDR$getPageSource()
  
  temp22 <- read_html(temp21[[1]]) %>% 
    html_elements('div.list-type038') %>% 
    html_elements('strong.tit-news') %>% 
    html_text() %>% 
    str_remove_all('\"') %>% 
    tibble()
  
  temp23 <- read_html(temp21[[1]]) %>% 
    html_elements('span.txt-time') %>% 
    html_text()
  
  temp23.1 <- paste0(str_sub(today(), 1, 4), '-', temp23) %>% 
    ymd_hm(tz = 'Asia/Seoul') %>% 
    tibble()
  
  temp24 <- read_html(temp21[[1]]) %>% 
    html_elements('a.tit-wrap') %>% 
    html_attr('href')
  
  temp24.1 <- paste0('https:', temp24[1:25]) %>% 
    tibble()
  
  temp25 <- tibble(제목 = temp22$., 시각 = temp23.1$., 링크 = temp24.1$.) %>% 
    relocate(시각, 링크, 제목)
  
  temp25.1 <- interval(temp25$시각, now(tz = 'Asia/Seoul')) %>% 
    as.period() < hours(PERIOD)
  
  if(sum(temp25.1) == 0) break
  
  YHNEWS <- rbind(YHNEWS, temp25)
  
}

YHNEWS <- YHNEWS %>% 
  mutate(시각 = as.character(시각))

# 3-6. 뉴시스(국제최신)

NEWSIS <- tibble()

for (i in 1:100) {
  
  URL_NEWSIS <- paste0('https://newsis.com/int/list/?cid=10100&scid=10101&page=', i)
  
  REMDR$navigate(URL_NEWSIS)
  
  temp26 <- REMDR$getPageSource()
  
  temp27 <- read_html(temp26[[1]]) %>% 
    html_elements('p.tit') %>% 
    html_elements('a') %>% 
    html_text() %>% 
    str_remove_all('\"') %>% 
    tibble() %>% 
    slice(2:21)
  
  temp28 <- read_html(temp26[[1]]) %>% 
    html_elements('p.time') %>% 
    html_text()
  
  temp28.1 <- temp28[1:20] %>% 
    str_split('기자', simplify = TRUE) %>% 
    as.data.frame() %>% 
    tibble()
  
  temp28.2 <- temp28.1$V2 %>% 
    str_replace_all('\\.', '-') %>% 
    ymd_hms(tz = 'Asia/Seoul') %>% 
    as.data.frame() %>% 
    tibble()
  
  temp29 <- read_html(temp26[[1]]) %>% 
    html_elements('p.tit') %>% 
    html_elements('a') %>% 
    html_attr('href')
  
  temp29.1 <- paste0('https://newsis.com', temp29[2:21]) %>% 
    tibble()
  
  temp30 <- tibble(제목 = temp27$., 기자 = temp28.1$V1, 시각 = temp28.2$., 링크 = temp29.1$.) %>% 
    relocate(시각, 기자, 링크, 제목)
  
  temp30.1 <- interval(temp30$시각, now(tz = 'Asia/Seoul')) %>% 
    as.period() < hours(PERIOD)
  
  if(sum(temp30.1) == 0) break
  
  NEWSIS <- rbind(NEWSIS, temp30)
  
}

NEWSIS <- NEWSIS %>% 
  mutate(시각 = as.character(시각))

# 3-7. 뉴스1(국제)

NEWS1 <- tibble()

URL_NEWS1 <- 'https://www.news1.kr/categories/?31'

REMDR$navigate(URL_NEWS1)

for (i in 2:10) {
  
  pagebutton <- REMDR$findElement(using = 'xpath', value = paste0('//*[@id="content"]/nav/ul/li[', i, ']'))
  pagebutton$clickElement()

  temp31 <- REMDR$getPageSource()
  
  temp32 <- read_html(temp31[[1]]) %>% 
    html_elements('h3.tit') %>% 
    html_text() %>% 
    str_remove_all('\"') %>% 
    tibble() %>% 
    slice(2:21)
  
  temp33 <- read_html(temp31[[1]]) %>% 
    html_elements('div.time') %>% 
    html_text() %>% 
    tibble()
  
  temp33.1 <- read_html(temp31[[1]]) %>% 
    html_elements('div.byline') %>% 
    html_text() %>% 
    str_remove_all(' 기자') %>% 
    tibble()
  
  temp34 <- read_html(temp31[[1]]) %>% 
    html_elements('li.article') %>% 
    html_elements('a') %>% 
    html_attr('onclick') %>% 
    tibble() %>% 
    distinct()
  
  temp34.1 <- temp34$. %>% 
    str_remove('goDetail') %>% 
    str_remove('\\(') %>% 
    str_remove('\\)') %>% 
    str_remove_all("'")
  
  temp34.2 <- paste0('https://news1.kr', temp34.1) %>% 
    tibble()
  
  temp35 <- tibble(제목 = temp32$., 기자 = temp33.1$., 시각 = temp33$., 링크 = temp34.2$.) %>% 
    relocate(시각, 기자, 링크, 제목)
  
  temp35.1 <- temp35$시각 %in% paste0(1:PERIOD, '시간전')
  
  temp35.2 <- temp35$시각 %in% paste0(1:60, '분전')
  
  if(sum(temp35.1, temp35.2) == 0) break
  
  NEWS1 <- rbind(NEWS1, temp35)
  
}

# 4. close

REMDR$close()

terminalKill(terminalList())

# 5. export

write.xlsx(x = list(INFOMAX, EBN, NEWSPIM, GLOBALECO, YHNEWS, NEWSIS, NEWS1), 
           sheetName = c('연합인포맥스(전체)', '산업경제신문(전체)', '뉴스핌(글로벌)', '글로벌이코노믹(국제)',
                         '연합뉴스(세계)', '뉴시스(국제최신)', '뉴스1(국제)'), 
           file = paste(today(), '뉴스.xlsx'), overwrite = TRUE)

# 6. summary 

temp36 <- nrow(INFOMAX) + nrow(EBN) + nrow(NEWSPIM) + nrow(GLOBALECO) + nrow(YHNEWS) + nrow(NEWSIS) + nrow(NEWS1)
temp37 <- paste0('안녕하세요 위원님, ', now() - hours(PERIOD), ' 부터 지금까지 작성된 기사 ', temp36 ,' 개를 모았습니다.')
temp38 <- paste0('RStudio를 종료하고, ', FOLDER, ' 폴더 내 [', today(), ' 뉴스.xlsx] 파일을 확인해주세요.')

print(c(temp37, temp38)) # Console 탭을 눌러주세요
```

## 5. mzyoon

네이버 상세검색으로 뉴스 링크 수집하기

```{r, eval = FALSE}
# mzyoon

# 22.04.14 updated

# 1. setting

setwd('C:/Users/KIET/Documents/GitHub/KIET/mzyoon') # working directory

library(tidyverse) # 데이터 핸들링
library(rstudioapi) # RStudio Terminal 탭 사용
library(RSelenium) # chrome 자동화
library(rvest) # html 데이터 읽기
library(lubridate) # 날짜
library(openxlsx) # 엑셀 로드, 출력

# 2. selenium

TERM_COMMAND <- 'java -Dwebdriver.gecko.driver="geckodriver.exe" -jar selenium-server-standalone-4.0.0-alpha-1.jar -port 4445'
terminalExecute(command = TERM_COMMAND)
REMDR = remoteDriver(port = 4445, browserName = 'chrome') # 어려운 멘트 다 됐고

REMDR$open() # 크롬 오픈

# 3. crawling

# 3-1. 검색창에서 간보기

TODAY <- today(tzone = 'Asia/Seoul') %>%
  str_replace_all('-', '.') # URL에서 오늘 날짜 설정해주기 위함

URL <- paste0('https://search.naver.com/search.naver',
              '?where=news',
              '&query=경제자유구역', # 검색어
              '&sm=tab_opt',
              '&sort=0',
              '&photo=0',
              '&field=0',
              '&pd=3',
              '&ds=', '1990.01.01', # 시작시점
              '&de=', TODAY, # 종료시점 = 오늘
              '&docid=',
              '&related=0',
              '&mynews=0',
              '&office_type=0',
              '&office_section_code=0',
              '&news_office_checked=',
              '&nso=so%3Ar%2Cp%3A',
              'from', '19900101', 'to', str_remove_all(TODAY, '\\.'), # 시작, 종료시점
              '&is_sug_officeid=0')

REMDR$navigate(URL) # 네이버 검색창으로 가서

temp1 <- REMDR$getPageSource() # 페이지 정보 싹 긁어온 다음

temp2 <- read_html(temp1[[1]]) %>% # html 데이터를 읽는데
  html_elements('div.sc_page_inner') %>% # 페이지 버튼 1~10 있는 파트에서
  html_elements('a.btn') %>% # 버튼 하나하나에 숨겨진
  html_attr('href') # 링크를 따온다

BUTTON_YOUNG <- paste0('https://search.naver.com/search.naver', temp2) # 링크 본체를 붙여서 저장

DATA <- tibble() # 데이터를 담을 빈 그릇

# 3-2. 초반 페이지 스캔

for (i in 1:6) { # 1-6 페이지
  
  REMDR$navigate(BUTTON_YOUNG[i]) # i번째 버튼을 눌러서
  
  temp.1 <- REMDR$getPageSource() # 역시 페이지 정보 스캔
  
  LINK <- read_html(temp.1[[1]]) %>% 
    html_elements('div.group_news') %>% # 뉴스 섹션에서
    html_elements('div.news_area') %>% # 개별 뉴스 파트마다 있는
    html_elements('a.news_tit') %>% # 뉴스 타이틀(누르면 접속)을 대상으로
    html_attr('href') # 링크를 가져오자
  
  PRESS <- read_html(temp.1[[1]]) %>% 
    html_elements('div.group_news') %>% 
    html_elements('div.news_area') %>% 
    html_elements('a.info.press') %>% # 언론 이름 파트
    html_text() # 그 중에서 텍스트로 된 것만 추출
  
  DATE <- read_html(temp.1[[1]]) %>% 
    html_elements('div.group_news') %>% 
    html_elements('div.news_area') %>% 
    html_elements('span.info') %>% # 날짜 파트
    html_text()
  
  DATE_CHECKER <- as.logical(DATE %>% str_detect('\\.') + 
                             DATE %>% str_detect('전')) # 간혹 'A면 13단' 처럼 날짜 아닌 텍스트도 잡혀서 거르기 위함
  
  temp.2 <- tibble(PRESS = PRESS, DATE = DATE[DATE_CHECKER], LINK = LINK) # 모은 데이터를 묶어서
  
  DATA <- rbind(temp.2, DATA) # 데이터 그릇에 차곡차곡 저장(반복하면 쌓임)
  
} 

# 6페이지에 돌입하자, MAX가 10에서 11로 하나 늘어남
# 서수적으로 보면 6번째 버튼을 계속 클릭하면 되겠다고 판단

# 3-3. 이후 페이지 스캔

for (k in 1:24) { # 7-30 페이지
  
  temp3 <- REMDR$getPageSource()
  
  temp4 <- read_html(temp3[[1]]) %>% 
    html_elements('div.sc_page_inner') %>% 
    html_elements('a.btn') %>% 
    html_attr('href')
  
  temp.1 <- REMDR$getPageSource()
  
  LINK <- read_html(temp.1[[1]]) %>% 
    html_elements('div.group_news') %>% 
    html_elements('div.news_area') %>% 
    html_elements('a.news_tit') %>% 
    html_attr('href')
  
  PRESS <- read_html(temp.1[[1]]) %>% 
    html_elements('div.group_news') %>% 
    html_elements('div.news_area') %>% 
    html_elements('a.info.press') %>% 
    html_text()
  
  DATE <- read_html(temp.1[[1]]) %>% 
    html_elements('div.group_news') %>% 
    html_elements('div.news_area') %>% 
    html_elements('span.info') %>% 
    html_text() 
  
  DATE_CHECKER <- as.logical(DATE %>% str_detect('\\.') + DATE %>% str_detect('전'))
  
  temp.2 <- tibble(PRESS = PRESS, DATE = DATE[DATE_CHECKER], LINK = LINK)
  
  DATA <- rbind(temp.2, DATA) # 여기까진 다 비슷한데
  
  BUTTON_OLD <- paste0('https://search.naver.com/search.naver', temp4) # 버튼 정보를 새로 따와야 함
  
  REMDR$navigate(BUTTON_OLD[6]) # 매번 6번째 버튼을 눌러서(다음 페이지로) 이동
  
}

# 4. export

write.xlsx(DATA, 'mzyoon.xlsx') # 엑셀 파일로 저장
```

## 6. dobby

자동퇴근 프로그램

```{r, eval = FALSE}
# dobby

# 22.04.19 updated

# 0. what do you need

FOLDER = paste0('C:/Users/', Sys.info()['user'], '/Documents/GitHub/KIET/dobby')

WHEN <- '17:45:00'
ID <- '21032'
PW <- '21032961228'
SHUTDOWN <- 'YES'

# 1. setting

setwd(FOLDER)

library(tidyverse)
library(rstudioapi)
library(RSelenium)
library(rvest)
library(lubridate)

# 2. selenium

TERM_COMMAND <- 'java -Dwebdriver.gecko.driver="geckodriver.exe" -jar selenium-server-standalone-4.0.0-alpha-1.jar -port 4445'
terminalExecute(command = TERM_COMMAND)
REMDR = remoteDriver(port = 4445, browserName = 'chrome')

REMDR$open()

# 3. commute check

for (i in 1:100) {
  
  # 3-1. time to go
  
  NOW <- now(tzone = 'Asia/Seoul')
  TODAY <- today(tzone = 'Asia/Seoul')
  GOHOME <- paste0(TODAY, ' ', WHEN) %>% ymd_hms(tz = 'Asia/Seoul')
  
  if(GOHOME > NOW){
    
    REMDR$navigate('https://www.kiet.re.kr/kiet_web/main/')
    
    print(paste0('자동퇴근 프로그램이 작동 중입니다.', ' (현재 : ', i, ' 회차)'))
    
    print('퇴근까지 남은 시간을 알려드립니다.')
    
    print(GOHOME - NOW)
    
  }
  
  if(GOHOME < NOW){
    
    # 3-2. login
    
    REMDR$navigate('https://ep.kiet.re.kr/index.do')
    
    BUTTON_LOGIN <- REMDR$findElement('xpath', '//*[@id="f_login"]/ul/li[3]')
    TEXT_ID <- REMDR$findElement('xpath', '//*[@id="loginId"]')
    TEXT_PW <- REMDR$findElement('xpath', '//*[@id="pwd"]')
    
    TEXT_ID$sendKeysToElement(list(ID))
    TEXT_PW$sendKeysToElement(list(PW))
    BUTTON_LOGIN$clickElement()
    
    # 3-3. leave
    
    temp <- REMDR$findElement('name', 'mainFrame')
    
    REMDR$switchToFrame(temp)
    
    BUTTON_LEAVE <- REMDR$findElement('xpath', '//*[@id="left"]/div[1]/a[2]')
    
    BUTTON_LEAVE$clickElement()
    
    # 3-4. turn off
    
    ifelse(SHUTDOWN == 'YES', system('shutdown -s'), '전기를 아낍시다.')
    
    # 3-5. quit
    
    q()
    
  }
  
  Sys.sleep(300 + rnorm(n = 1, mean = 10, sd = 2))
  
}
```
